# -*- coding: utf-8 -*-
"""RAG Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FckzEIM1qZ4DSffZ8UJ8336hRmM4qXLn
"""

!pip -q install -U \
  langchain langchain-community langchain-core \
  faiss-cpu sentence-transformers \
  transformers accelerate \
  pandas openpyxl pypdf

import pathlib
pathlib.Path("rag_pipeline").mkdir(parents=True, exist_ok=True)
pathlib.Path("data").mkdir(parents=True, exist_ok=True)
pathlib.Path("vector_store").mkdir(parents=True, exist_ok=True)

with open("rag_pipeline/__init__.py", "w") as f:
    f.write("")

embeddings_py = r'''
from __future__ import annotations
from typing import Optional
from langchain_community.embeddings import HuggingFaceBgeEmbeddings

_EMBEDDINGS: Optional[HuggingFaceBgeEmbeddings] = None

def get_embedding_model(
    model_name: str = "BAAI/bge-small-en-v1.5",
    device: Optional[str] = None,
) -> HuggingFaceBgeEmbeddings:
    """
    Low-memory embedding loader with caching.
    """
    global _EMBEDDINGS

    if device is None:
        try:
            import torch
            device = "cuda" if torch.cuda.is_available() else "cpu"
        except Exception:
            device = "cpu"

    if _EMBEDDINGS is None:
        _EMBEDDINGS = HuggingFaceBgeEmbeddings(
            model_name=model_name,
            model_kwargs={"device": device},
            encode_kwargs={"normalize_embeddings": True},
        )
    return _EMBEDDINGS
'''
with open("rag_pipeline/embeddings.py", "w") as f:
    f.write(embeddings_py)

print("Wrote embeddings.py")

vector_store_py = r'''
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Optional

from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_community.vectorstores import FAISS

DEFAULT_INDEX_DIR = Path("vector_store/faiss_index")

@dataclass
class FaissVectorStore:
    embedding: Embeddings
    index: Optional[FAISS] = None

    def add_documents(self, documents: List[Document]) -> None:
        if not documents:
            return
        if self.index is None:
            self.index = FAISS.from_documents(documents, self.embedding)
        else:
            self.index.add_documents(documents)

    def similarity_search(self, query: str, k: int = 3) -> List[Document]:
        if self.index is None:
            raise ValueError("Vector store is empty. Call add_documents() first.")
        return self.index.similarity_search(query, k=k)

    def save_local(self, index_dir: Path = DEFAULT_INDEX_DIR) -> None:
        if self.index is None:
            raise ValueError("No FAISS index to save.")
        index_dir.mkdir(parents=True, exist_ok=True)
        self.index.save_local(str(index_dir))

    @classmethod
    def load_local(
        cls,
        embedding: Embeddings,
        index_dir: Path = DEFAULT_INDEX_DIR,
    ) -> "FaissVectorStore":
        if not index_dir.exists():
            raise FileNotFoundError(f"Index folder not found: {index_dir}")
        store = cls(embedding=embedding)
        store.index = FAISS.load_local(
            str(index_dir),
            embedding,
            allow_dangerous_deserialization=True,
        )
        return store

    @classmethod
    def build_from_iterable(
        cls,
        embedding: Embeddings,
        docs_iter: Iterable[Document],
        batch_size: int = 64,
        save_every_batches: int = 10,
        index_dir: Path = DEFAULT_INDEX_DIR,
    ) -> "FaissVectorStore":
        """
        Memory-safe FAISS builder:
        - consumes documents in small batches
        - incrementally adds to FAISS
        - periodically saves to disk
        """
        store = cls(embedding=embedding)

        batch: List[Document] = []
        batch_count = 0

        for doc in docs_iter:
            batch.append(doc)
            if len(batch) >= batch_size:
                store.add_documents(batch)
                batch.clear()
                batch_count += 1

                if batch_count % save_every_batches == 0:
                    try:
                        store.save_local(index_dir)
                    except Exception:
                        pass

        if batch:
            store.add_documents(batch)

        # final save
        try:
            store.save_local(index_dir)
        except Exception:
            pass

        return store
'''
with open("rag_pipeline/vector_store.py", "w") as f:
    f.write(vector_store_py)

print("Wrote vector_store.py")

retriever_py = r'''
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List

try:
    from langchain_core.documents import Document
except Exception:
    from langchain.schema import Document

@dataclass
class SimpleRetriever:
    vector_store: Any

    def retrieve(self, query: str, k: int = 3) -> List[Document]:
        if k < 3:
            k = 3
        return self.vector_store.similarity_search(query, k=k)

    @staticmethod
    def build_sources(docs: List[Document], max_snippet_chars: int = 220) -> List[Dict[str, Any]]:
        sources: List[Dict[str, Any]] = []
        for d in docs:
            meta = getattr(d, "metadata", {}) or {}
            content = getattr(d, "page_content", "") or ""
            sources.append({
                "id": meta.get("source", "unknown"),
                "page": meta.get("page"),
                "country": meta.get("country"),
                "year": meta.get("year"),
                "snippet": content[:max_snippet_chars],
            })
        return sources
'''
with open("rag_pipeline/retriever.py", "w") as f:
    f.write(retriever_py)

print("Wrote retriever.py")

rag_py = r'''
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List

from .retriever import SimpleRetriever

try:
    from langchain_core.documents import Document
except Exception:
    from langchain.schema import Document


SYSTEM_PROMPT = """
You MUST answer ONLY using the provided context.
If the answer cannot be found in the context, say:
"I don't have enough information in the provided data to answer that."
""".strip()


def build_context_block(docs: List[Document]) -> str:
    parts: List[str] = []
    for i, d in enumerate(docs, start=1):
        meta = getattr(d, "metadata", {}) or {}
        content = getattr(d, "page_content", "") or ""
        src = meta.get("source", "unknown")
        page = meta.get("page")
        country = meta.get("country")
        year = meta.get("year")

        header_bits = [f"Source={src}"]
        if page is not None:
            header_bits.append(f"page={page}")
        if country:
            header_bits.append(f"country={country}")
        if year:
            header_bits.append(f"year={year}")

        parts.append(f"[{i}] " + ", ".join(header_bits) + "\n" + content)

    return "\n\n".join(parts)


def dummy_llm(prompt: str, **kwargs):
    return [{
        "generated_text": prompt + "\nI don't have enough information in the provided data to answer that."
    }]


def get_hf_llm(model_name: str = "Qwen/Qwen1.5-4B-Chat", max_new_tokens: int = 256):
    """
    Load a HF local model.
    Use ONLY after your index + retrieval works.
    """
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

    try:
        import torch
        has_cuda = torch.cuda.is_available()
        dtype = torch.float16 if has_cuda else torch.float32
        device_map = "auto" if has_cuda else None
    except Exception:
        dtype = None
        device_map = None

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype=dtype,
        device_map=device_map,
    )
    gen = pipeline("text-generation", model=model, tokenizer=tokenizer)

    def _llm(prompt: str, **kwargs):
        return gen(prompt, max_new_tokens=kwargs.get("max_new_tokens", max_new_tokens), do_sample=False)

    return _llm


@dataclass
class RAGPipeline:
    retriever: SimpleRetriever
    llm: Any = dummy_llm

    def generate_answer(self, question: str, docs: List[Document]) -> str:
        context = build_context_block(docs)

        prompt = f"""
{SYSTEM_PROMPT}

Context:
{context}

Question:
{question}

Answer:
""".strip()

        out = self.llm(prompt, max_new_tokens=256, do_sample=False)

        if isinstance(out, list) and out and isinstance(out[0], dict):
            text = out[0].get("generated_text", "")
            if text.startswith(prompt):
                text = text[len(prompt):].strip()
            return text.strip()

        return str(out).strip()

    def ask(self, question: str, k: int = 3) -> Dict[str, Any]:
        docs = self.retriever.retrieve(question, k=k)
        answer = self.generate_answer(question, docs)
        sources = self.retriever.build_sources(docs)
        return {"answer": answer, "sources": sources}
'''
with open("rag_pipeline/rag.py", "w") as f:
    f.write(rag_py)

print("Wrote rag.py")

ingest_lowmem_py = r'''
from __future__ import annotations

from pathlib import Path
from typing import Generator, Iterable, List, Optional

import pandas as pd

try:
    from langchain_core.documents import Document
except Exception:
    from langchain.schema import Document


def iter_csv_documents(
    csv_path: str | Path,
    chunk_rows: int = 2000,
    countries: Optional[List[str]] = None,
    min_year: Optional[int] = None,
) -> Generator[Document, None, None]:
    """
    Memory-safe CSV ingestion:
    - read with pandas chunksize
    - yield small per-row Documents
    """
    csv_path = Path(csv_path)

    for chunk in pd.read_csv(csv_path, chunksize=chunk_rows):
        # filters
        if countries and "country" in chunk.columns:
            chunk = chunk[chunk["country"].isin(countries)]
        if min_year is not None and "year" in chunk.columns:
            chunk = chunk[chunk["year"] >= min_year]

        for _, row in chunk.iterrows():
            country = row.get("country", None)
            year = row.get("year", None)

            # Build a compact text line (avoid huge strings)
            fields = []
            if "co2" in chunk.columns:
                fields.append(f"co2={row.get('co2')}")
            if "gdp_per_capita" in chunk.columns:
                fields.append(f"gdp_per_capita={row.get('gdp_per_capita')}")
            if "temp_anomaly" in chunk.columns:
                fields.append(f"temp_anomaly={row.get('temp_anomaly')}")

            text = f"{country} {year}: " + ", ".join(fields)

            yield Document(
                page_content=text,
                metadata={
                    "source": csv_path.name,
                    "country": country,
                    "year": year,
                }
            )


def iter_pdf_documents(
    pdf_path: str | Path,
    chunk_chars: int = 1200,
    overlap: int = 150,
) -> Generator[Document, None, None]:
    """
    Light PDF ingestion without holding everything at once.
    """
    from pypdf import PdfReader

    pdf_path = Path(pdf_path)
    reader = PdfReader(str(pdf_path))

    for i, page in enumerate(reader.pages, start=1):
        text = page.extract_text() or ""
        if not text.strip():
            continue

        # naive character chunking (low overhead)
        start = 0
        while start < len(text):
            end = min(len(text), start + chunk_chars)
            chunk_text = text[start:end]

            yield Document(
                page_content=chunk_text,
                metadata={"source": pdf_path.name, "page": i}
            )

            start = end - overlap if overlap > 0 else end
'''
with open("rag_pipeline/ingest_lowmem.py", "w") as f:
    f.write(ingest_lowmem_py)

print("Wrote ingest_lowmem.py")

import shutil, pathlib

pathlib.Path("data").mkdir(parents=True, exist_ok=True)

src = "/content/vector_store/etl_cleaned_dataset.xlsx"
dst = "data/etl_cleaned_dataset.xlsx"

if pathlib.Path(src).exists():
    shutil.copy(src, dst)
    print("✅ copied:", src, "->", dst)
else:
    print("❌ still not found:", src)

import pandas as pd
df = pd.read_excel("data/etl_cleaned_dataset.xlsx")
df.to_csv("data/etl_cleaned_dataset.csv", index=False)
print("converted to csv")

import pathlib, sys, textwrap, os

# 1) Ensure /content is importable
if "/content" not in sys.path:
    sys.path.append("/content")

# 2) Ensure package folder + __init__.py
PKG = pathlib.Path("/content/rag_pipeline")
PKG.mkdir(parents=True, exist_ok=True)
init_file = PKG / "__init__.py"
if not init_file.exists():
    init_file.write_text("")

# 3) Ensure ingest_lowmem.py exists (if you already have it, this won't hurt)
ingest_file = PKG / "ingest_lowmem.py"
if not ingest_file.exists():
    ingest_file.write_text(textwrap.dedent("""
        from __future__ import annotations
        from pathlib import Path
        from typing import Generator, List, Optional
        import pandas as pd

        try:
            from langchain_core.documents import Document
        except Exception:
            from langchain.schema import Document


        def iter_csv_documents(
            csv_path: str | Path,
            chunk_rows: int = 2000,
            countries: Optional[List[str]] = None,
            min_year: Optional[int] = None,
        ) -> Generator[Document, None, None]:
            csv_path = Path(csv_path)

            for chunk in pd.read_csv(csv_path, chunksize=chunk_rows):
                if countries and "country" in chunk.columns:
                    chunk = chunk[chunk["country"].isin(countries)]
                if min_year is not None and "year" in chunk.columns:
                    chunk = chunk[chunk["year"] >= min_year]

                for _, row in chunk.iterrows():
                    country = row.get("country", None)
                    year = row.get("year", None)

                    fields = []
                    if "co2" in chunk.columns:
                        fields.append(f"co2={row.get('co2')}")
                    if "gdp_per_capita" in chunk.columns:
                        fields.append(f"gdp_per_capita={row.get('gdp_per_capita')}")
                    if "temp_anomaly" in chunk.columns:
                        fields.append(f"temp_anomaly={row.get('temp_anomaly')}")

                    text = f"{country} {year}: " + ", ".join(fields)

                    yield Document(
                        page_content=text,
                        metadata={"source": csv_path.name, "country": country, "year": year}
                    )
    """).strip() + "\n")

# 4) Write build_index_lowmem.py (this is the one you're importing)
build_index_file = PKG / "build_index_lowmem.py"
build_index_file.write_text(textwrap.dedent("""
    from __future__ import annotations

    from pathlib import Path
    from typing import List

    from .embeddings import get_embedding_model
    from .vector_store import FaissVectorStore
    from .ingest_lowmem import iter_csv_documents


    def build_index_from_csv(
        csv_path: str | Path,
        index_dir: str | Path = "vector_store/faiss_index",
        countries: List[str] | None = None,
        min_year: int | None = None,
        chunk_rows: int = 2000,
        batch_size: int = 64,
    ):
        emb = get_embedding_model()

        docs_iter = iter_csv_documents(
            csv_path,
            chunk_rows=chunk_rows,
            countries=countries,
            min_year=min_year,
        )

        store = FaissVectorStore.build_from_iterable(
            embedding=emb,
            docs_iter=docs_iter,
            batch_size=batch_size,
            save_every_batches=10,
            index_dir=Path(index_dir),
        )
        return store
""").strip() + "\n")

# 5) Show what's inside your package now
print("✅ rag_pipeline files:", [p.name for p in PKG.glob("*.py")])
print("✅ __init__.py exists?", init_file.exists())
print("✅ build_index_lowmem.py exists?", build_index_file.exists())
print("Current working dir:", os.getcwd())

from rag_pipeline.build_index_lowmem import build_index_from_csv

store = build_index_from_csv(
    "data/etl_cleaned_dataset.csv",
    countries=["United States", "Canada"],
    min_year=1990,
    chunk_rows=2000,   # CSV read chunks
    batch_size=64      # embedding batches
)

print("Index built.")

from rag_pipeline.embeddings import get_embedding_model
from rag_pipeline.vector_store import FaissVectorStore
from rag_pipeline.retriever import SimpleRetriever
from rag_pipeline.rag import RAGPipeline, dummy_llm

emb = get_embedding_model()
vs = FaissVectorStore.load_local(embedding=emb)  # load from disk

retriever = SimpleRetriever(vector_store=vs)
rag = RAGPipeline(retriever=retriever, llm=dummy_llm)

rag.ask("What does the dataset say about US CO2 around 2000?", k=3)